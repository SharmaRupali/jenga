{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from jenga.basis import Dataset\n",
    "from jenga.corruptions.generic import MissingValues, SwappedValues\n",
    "from jenga.corruptions.numerical import Scaling, GaussianNoise\n",
    "from jenga.cleaning.ppp import PipelinePerformancePrediction\n",
    "from jenga.cleaning.outlier_detection import NoOutlierDetection, PyODKNN, PyODIsolationForest\n",
    "from jenga.cleaning.imputation import NoImputation, MeanModeImputation, DatawigImputation\n",
    "from jenga.cleaning.clean import Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(dataset_name, learner, param_grid, corruptions, cleaners):\n",
    "    \n",
    "    ## dataset\n",
    "    dataset = Dataset(seed, dataset_name)\n",
    "    \n",
    "    all_data = dataset.all_data\n",
    "    attribute_names = dataset.attribute_names\n",
    "    attribute_types = dataset.attribute_types\n",
    "    \n",
    "    ## categorical and numerical features\n",
    "    categorical_columns = dataset.categorical_columns\n",
    "    numerical_columns = dataset.numerical_columns\n",
    "    print(f\"Found {len(categorical_columns)} categorical and {len(numerical_columns)} numeric features \\n\")\n",
    "    \n",
    "    ## train and test data\n",
    "    df_train, lab_train, df_test, lab_test = dataset.get_train_test_data()\n",
    "    \n",
    "    \n",
    "    ## pipeline performance prediction (ppp)\n",
    "    ppp = PipelinePerformancePrediction(seed, df_train, lab_train, df_test, lab_test, categorical_columns, numerical_columns, learner, param_grid, corruptions)\n",
    "    ppp_model = ppp.fit_ppp(df_train)\n",
    "    \n",
    "    ## generate corrpted data\n",
    "    df_corrupted, perturbations, cols_perturbed, summary_col_corrupt = ppp.get_corrupted(df_test, corruptions)\n",
    "    \n",
    "    ## cleaning\n",
    "    clean = Clean(df_train, df_corrupted, categorical_columns, numerical_columns, ppp, ppp_model, cleaners)\n",
    "    df_cleaned, corrupted_score_ppp, cleaner_scores_ppp, summary_cleaners = clean(df_train, df_corrupted)\n",
    "    \n",
    "    ## results\n",
    "    result = {\n",
    "        'ppp_score_model': ppp.predict_score_ppp(ppp_model, df_test),\n",
    "        'ppp_score_corrupted': corrupted_score_ppp,\n",
    "        'ppp_score_cleaned': np.array(cleaner_scores_ppp).max(),\n",
    "        'ppp_scores_cleaners': cleaner_scores_ppp\n",
    "    }\n",
    "    #print('\\n'.join([f'{key}:{val}' for key, val in result.items()]))\n",
    "    \n",
    "    ## summary\n",
    "    summary = {\n",
    "        'dataset': dataset,\n",
    "        'model': learner,\n",
    "        'corruptions': summary_col_corrupt,\n",
    "        'cleaners': summary_cleaners,\n",
    "        'result': result\n",
    "    }\n",
    "    print('\\n\\n\\n\\n'.join([f'{key}:{val}' for key, val in summary.items()]))\n",
    "    \n",
    "    return summary #summary_col_corrupt, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    'credit-g',\n",
    "    'heart-statlog',\n",
    "    'parkinsons'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model parameters\n",
    "learner = SGDClassifier(max_iter=1000)\n",
    "param_grid = {\n",
    "    'learner__loss': ['log'],\n",
    "    'learner__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'learner__alpha': [0.0001, 0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "## make dict of multiple leraners and corresponding param_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corruptions = [MissingValues, SwappedValues, Scaling, GaussianNoise]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaners = [\n",
    "    (NoOutlierDetection, MeanModeImputation),\n",
    "    (PyODKNN, NoImputation),\n",
    "    (PyODKNN, MeanModeImputation),\n",
    "    (PyODIsolationForest, NoImputation),\n",
    "    (PyODIsolationForest, MeanModeImputation)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# results = []\n",
    "# for _ in range(5):\n",
    "#     results.append(run_experiment(random.choice(datasets), learner, param_grid, corruptions, cleaners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "individual_results = []\n",
    "results = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    individual_results.append(run_experiment(dataset, learner, param_grid, corruptions, cleaners))\n",
    "    results.append(individual_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
